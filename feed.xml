<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2024-12-23T07:22:19+00:00</updated><id>/feed.xml</id><title type="html">Lab IRoS, Fasilkom UI</title><subtitle>Lab IRoS ini berfokus pada penelitian yang berkaitan dengan pengembangan sistem robotik cerdas. Area kajiannya mencakup sistem sensor multimodal, model-model representasi data dan analitika untuk pengembilan keputusan pada sistem robotika, serta sistem aktuator robotika dinamis.</subtitle><entry><title type="html">Example post 3</title><link href="/2023/02/23/example-post-3.html" rel="alternate" type="text/html" title="Example post 3" /><published>2023-02-23T00:00:00+00:00</published><updated>2024-12-23T07:14:08+00:00</updated><id>/2023/02/23/example-post-3</id><content type="html" xml:base="/2023/02/23/example-post-3.html"><![CDATA[<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.</p>]]></content><author><name>john-doe</name></author><category term="biology," /><category term="medicine" /><summary type="html"><![CDATA[Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.]]></summary></entry><entry><title type="html">Gradient Descent Simplified</title><link href="/2023/01/21/example-post-2.html" rel="alternate" type="text/html" title="Gradient Descent Simplified" /><published>2023-01-21T00:00:00+00:00</published><updated>2024-12-23T07:14:08+00:00</updated><id>/2023/01/21/example-post-2</id><content type="html" xml:base="/2023/01/21/example-post-2.html"><![CDATA[<!-- excerpt start -->
<p>Gradient descent is an optimization algorithm used to minimize a function. Gradient descent works by iteratively moving in the direction opposite to the gradient of the function, with the step size determined by a hyperparameter called ‚Äúlearning rate‚Äù. Gradient descent is commonly used in machine learning to adjust the parameters of a model in order to minimize the loss function, especially in deep learning. In deep learning, gradient descent will adjust every trainable weights and biases of the model in which when these weights and biases are used, the loss function will be minimum.
<!-- excerpt end --></p>

<p>Mathematically speaking, gradient descent can be formulated as follow:
$$X_{t+1}=X_t - \alpha \nabla$$</p>

<p>where,
$X_{t+1}$ = new value of x after updated
$X_t$ = current value of x
$\alpha$ = learning rate
$\nabla$ = gradient with respect to x</p>

<p>There are several types of gradient descent, including:</p>
<ol>
  <li>batch gradient descent</li>
  <li>stochastic gradient descent (SGD)</li>
  <li>mini-batch gradient descent</li>
</ol>

<h1 id="batch-gradient-descent">Batch gradient descent</h1>
<p>Batch gradient descent is a type of gradient descent that update the parameters after forward and backward pass through the entire dataset. It is called ‚Äúbatch‚Äù gradient descent because it uses the entire dataset to compute the gradient of the loss function at each iteration. Batch gradient descent can be formulated as follows:
$$X_{t+1}=X_t - \alpha \sum_{1}^{n}\nabla$$</p>]]></content><author><name>Mgs M Luthfi Ramadhan</name></author><category term="Data Science" /><category term="Artificial Intelligence" /><category term="Machine Learning" /><category term="NLP" /><category term="Data" /><summary type="html"><![CDATA[Gradient descent is an optimization algorithm used to minimize a function. Gradient descent works by iteratively moving in the direction opposite to the gradient of the function, with the step size determined by a hyperparameter called ‚Äúlearning rate‚Äù. Gradient descent is commonly used in machine learning to adjust the parameters of a model in order to minimize the loss function, especially in deep learning. In deep learning, gradient descent will adjust every trainable weights and biases of the model in which when these weights and biases are used, the loss function will be minimum.]]></summary></entry><entry><title type="html">TF-IDF Simplified</title><link href="/2021/01/20/example-post-1.html" rel="alternate" type="text/html" title="TF-IDF Simplified" /><published>2021-01-20T00:00:00+00:00</published><updated>2024-12-23T07:14:08+00:00</updated><id>/2021/01/20/example-post-1</id><content type="html" xml:base="/2021/01/20/example-post-1.html"><![CDATA[<!-- excerpt start -->
<p>Most machine learning algorithms are fulfilled with mathematical things such as statistics, algebra, calculus and etc. They expect the data to be numerical such as a 2-dimensional array with rows as instances and columns as features. The problem with natural language is that the data is in the form of raw text, so that the text needs to be transformed into a vector. The process of transforming text into a vector is commonly referred to as text vectorization. It‚Äôs a fundamental process in natural language processing because none of the machine learning algorithms understand a text, not even computers. Text vectorization algorithm namely TF-IDF vectorizer, which is a very popular approach for traditional machine learning algorithms can help in transforming text into vectors.</p>

<p>This article is originally published in <a href="https://medium.com/towards-data-science/tf-idf-simplified-aba19d5f5530">Towards Data Science</a> and is currently behind medium paywall. If you have medium subscription, please have a look at the original version.
<!-- excerpt end --></p>

<h1 id="tf-idf">TF-IDF</h1>
<p>Term frequency-inverse document frequency is a text vectorizer that transforms the text into a usable vector. It combines 2 concepts, Term Frequency (TF) and Document Frequency (DF).</p>

<p>The term frequency is the number of occurrences of a specific term in a document. Term frequency indicates how important a specific term in a document. Term frequency represents every text from the data as a matrix whose rows are the number of documents and columns are the number of distinct terms throughout all documents.
Document frequency is the number of documents containing a specific term. Document frequency indicates how common the term is.</p>

<p>Inverse document frequency (IDF) is the weight of a term, it aims to reduce the weight of a term if the term‚Äôs occurrences are scattered throughout all the documents. IDF can be calculated as follow:
$${idf}_i=\log(\frac{n}{df_i})$$</p>

<p>Where idf·µ¢ is the IDF score for term i, ${df}_i$ is the number of documents containing term i, and n is the total number of documents. The higher the DF of a term, the lower the IDF for the term. When the number of DF is equal to n which means that the term appears in all documents, the IDF will be zero, since $log(1)$ is zero, when in doubt just put this term in the stopword list because it doesn‚Äôt provide much information.</p>

<p>The TF-IDF score as the name suggests is just a multiplication of the term frequency matrix with its IDF, it can be calculated as follow:
$$w_{i,j}={tf}_{i,j} \times {idf}_i$$</p>

<p>Where $w_{i,j}$ is TF-IDF score for term i in document j, ${tf}_{i,j}$ is term frequency for term i in document j, and ${df}_i$ is IDF score for term i.</p>

<h1 id="example">Example</h1>
<p>Suppose we have 3 texts and we need to vectorize these texts using TF-IDF.
<img src="https://archive.ph/8Sh0J/bb34f145575d833d1a39e674b9683335a864ab16.webp" alt="Alt text" /></p>

<h2 id="step-1">Step 1</h2>
<p>Create a term frequency matrix where rows are documents and columns are distinct terms throughout all documents. Count word occurrences in every text.
<img src="https://archive.ph/8Sh0J/82150f914b997894d130ba4b50cc5722525c173c.webp" alt="Alt text" /></p>

<h2 id="step-2">Step 2</h2>
<p>Compute inverse document frequency (IDF) using the previously explained formula.
<img src="https://archive.ph/8Sh0J/60d918258fb0c9a2fc310fec6a1cdffe89580092.webp" alt="Alt text" /></p>

<h2 id="step-3">Step 3</h2>
<p>Multiply TF matrix with IDF respectively
<img src="https://archive.ph/8Sh0J/99ef38c9b1da12ef5cfae59545fb8a08ae60b142.webp" alt="Alt text" /></p>

<p>That‚Äôs it üòÉ! the text is now ready to feed into a machine learning algorithm.</p>

<h1 id="limitations">Limitations</h1>
<ol>
  <li>It is only useful as a lexical level feature.</li>
  <li>Synonymities are neglected.</li>
  <li>It doesn‚Äôt capture semantic.</li>
  <li>The highest TF-IDF score may not make sense with the topic of the document, since IDF gives high weight if the DF of a term is low.</li>
  <li>It neglects the sequence of the terms.</li>
</ol>

<h1 id="conclusion">Conclusion</h1>
<p>In order to process natural language, the text must be represented as a numerical feature. The process of transforming text into a numerical feature is called text vectorization. TF-IDF is one of the most popular text vectorizers, the calculation is very simple and easy to understand. It gives the rare term high weight and gives the common term low weight.</p>]]></content><author><name>Mgs M Luthfi Ramadhan</name></author><category term="Data Science" /><category term="Artificial Intelligence" /><category term="Machine Learning" /><category term="NLP" /><category term="Data" /><summary type="html"><![CDATA[Most machine learning algorithms are fulfilled with mathematical things such as statistics, algebra, calculus and etc. They expect the data to be numerical such as a 2-dimensional array with rows as instances and columns as features. The problem with natural language is that the data is in the form of raw text, so that the text needs to be transformed into a vector. The process of transforming text into a vector is commonly referred to as text vectorization. It‚Äôs a fundamental process in natural language processing because none of the machine learning algorithms understand a text, not even computers. Text vectorization algorithm namely TF-IDF vectorizer, which is a very popular approach for traditional machine learning algorithms can help in transforming text into vectors.]]></summary></entry></feed>